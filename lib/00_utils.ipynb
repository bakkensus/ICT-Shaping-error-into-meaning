{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d950b-50a6-4cbb-992f-c110bff00a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 00_core.ipynb - Unified Framework for CIFAR-10 Experiments\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. GLOBAL KONFIGURASJON & UTILS\n",
    "# ==========================================\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CIFAR10_CLASSES = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODELL (ResNet18 for CIFAR-10)\n",
    "# ==========================================\n",
    "def make_cifar_resnet18(num_classes=10):\n",
    "    \"\"\"Lager en ResNet18 tilpasset CIFAR-10 (mindre kernel i start, ingen maxpool).\"\"\"\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "# ==========================================\n",
    "# 3. SPESIALISERT LOGIKK (SBLS, DYNAMIC ETC.)\n",
    "# ==========================================\n",
    "\n",
    "# --- For Exp 2: SBLS ---\n",
    "def build_similarity_matrix_cifar10():\n",
    "    \"\"\"Håndlaget likhetsmatrise for SBLS.\"\"\"\n",
    "    C = len(CIFAR10_CLASSES)\n",
    "    S = torch.zeros(C, C)\n",
    "    animals = {2,3,4,5,6,7}\n",
    "    vehicles = {0,1,8,9}\n",
    "    for i in range(C):\n",
    "        for j in range(C):\n",
    "            if i == j: \n",
    "                S[i,j] = 0.0\n",
    "            elif (i in animals and j in animals) or (i in vehicles and j in vehicles):\n",
    "                S[i,j] = 1.0\n",
    "            else:\n",
    "                S[i,j] = 0.0\n",
    "    row_sums = S.sum(dim=1, keepdim=True).clamp_min(1e-8)\n",
    "    return S / row_sums\n",
    "\n",
    "def make_soft_targets_sbls(y, S, alpha=0.2, num_classes=10):\n",
    "    \"\"\"Lager soft targets on-the-fly basert på likhetsmatrise S.\"\"\"\n",
    "    B = y.shape[0]\n",
    "    T = torch.zeros(B, num_classes, device=y.device)\n",
    "    T.scatter_(1, y.view(-1,1), 1.0)\n",
    "    if alpha <= 0: return T\n",
    "    T = T * (1.0 - alpha)\n",
    "    rows = S[y]\n",
    "    row_sums = rows.sum(dim=1, keepdim=True)\n",
    "    fallback = (row_sums < 1e-7).float() # Hvis ingen naboer, uniform smoothing\n",
    "    if fallback.any():\n",
    "        uniform = torch.ones_like(rows) / (num_classes - 1)\n",
    "        uniform.scatter_(1, y.view(-1,1), 0.0)\n",
    "        rows = torch.where(fallback.bool(), uniform, rows)\n",
    "    T = T + alpha * rows\n",
    "    return T\n",
    "\n",
    "# --- For Exp 4/5: Dynamisk Target Update ---\n",
    "@torch.no_grad()\n",
    "def update_target_logits_dynamic(T, acc, T0=None, beta=10.0, lr=0.10, kappa=0.10, \n",
    "                                 min_diff=0.05, anchor_gamma=0.0, clamp_M=10.0):\n",
    "    \"\"\"\n",
    "    Oppdaterer mål-matrisen T basert på forskjell i presisjon (acc).\n",
    "    Novise (i) lærer av Ekspert (j) hvis acc[j] > acc[i].\n",
    "    \"\"\"\n",
    "    C = T.shape[0]\n",
    "    T_new = T.clone()\n",
    "    updates_made = 0\n",
    "    \n",
    "    for i in range(C): # Novise\n",
    "        for j in range(C): # Ekspert\n",
    "            if i == j: continue\n",
    "            \n",
    "            diff = acc[j] - acc[i]\n",
    "            if diff <= min_diff: continue\n",
    "            \n",
    "            # Autoritet sigmoid\n",
    "            p = torch.sigmoid(torch.tensor(beta * diff, device=T.device))\n",
    "            \n",
    "            expert_view = T[j, i]\n",
    "            novice_view = T[i, j]\n",
    "            \n",
    "            # Trekk novisen mot eksperten\n",
    "            change = lr * p * (expert_view - novice_view)\n",
    "            T_new[i, j] += change\n",
    "            \n",
    "            # Pushback (frastøting) på eksperten\n",
    "            if kappa > 0:\n",
    "                T_new[j, i] -= kappa * change\n",
    "            \n",
    "            updates_made += 1\n",
    "            \n",
    "    # Anker mot start-targets (T0)\n",
    "    if T0 is not None and anchor_gamma > 0:\n",
    "        T_new = (1.0 - anchor_gamma) * T_new + anchor_gamma * T0\n",
    "        \n",
    "    # Clamp for stabilitet\n",
    "    if clamp_M > 0:\n",
    "        T_new = T_new.clamp(min=-clamp_M, max=clamp_M)\n",
    "        \n",
    "    return T_new, updates_made\n",
    "\n",
    "@torch.no_grad()\n",
    "def enforce_target_dominance(logits_matrix):\n",
    "    \"\"\"Exp 5b: Swap hvis en annen klasse har høyere verdi enn 'sann' klasse.\"\"\"\n",
    "    C = logits_matrix.shape[0]\n",
    "    swaps_count = 0\n",
    "    for i in range(C):\n",
    "        row = logits_matrix[i]\n",
    "        val_max, idx_max = torch.max(row, dim=0)\n",
    "        if idx_max != i:\n",
    "            val_self = row[i].clone()\n",
    "            logits_matrix[i, i] = val_max\n",
    "            logits_matrix[i, idx_max] = val_self\n",
    "            swaps_count += 1\n",
    "    return logits_matrix, swaps_count\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_probability_cap(logits_matrix, max_p=0.80):\n",
    "    \"\"\"Exp 5: Tving maks sannsynlighet for diagonalen, redistribuer resten.\"\"\"\n",
    "    C = logits_matrix.shape[0]\n",
    "    probs = torch.softmax(logits_matrix, dim=1)\n",
    "    \n",
    "    for i in range(C):\n",
    "        p_self = probs[i, i]\n",
    "        if p_self > max_p:\n",
    "            probs[i, i] = max_p\n",
    "            others_mask = torch.ones(C, dtype=torch.bool, device=logits_matrix.device)\n",
    "            others_mask[i] = False\n",
    "            \n",
    "            current_others_sum = probs[i, others_mask].sum()\n",
    "            if current_others_sum > 1e-9:\n",
    "                scale = (1.0 - max_p) / current_others_sum\n",
    "                probs[i, others_mask] *= scale\n",
    "            else:\n",
    "                probs[i, others_mask] = (1.0 - max_p) / (C - 1)\n",
    "                \n",
    "    new_logits = torch.log(probs + 1e-9)\n",
    "    # Bevar magnitude range for stabilitet\n",
    "    old_max = logits_matrix.max(dim=1, keepdim=True).values\n",
    "    new_max = new_logits.max(dim=1, keepdim=True).values\n",
    "    final_logits = new_logits - new_max + old_max\n",
    "    return final_logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def weights_from_acc(acc, gamma=1.0, eps=1e-6):\n",
    "    \"\"\"Exp 3b/4/5: Vekter klasser basert på presisjon. Høyere acc = høyere vekt.\"\"\"\n",
    "    w = (acc.clamp_min(0.0) + eps).pow(gamma)\n",
    "    w = w / w.mean().clamp_min(1e-8)\n",
    "    return w\n",
    "\n",
    "# ==========================================\n",
    "# 4. LOSS FUNCTIONS\n",
    "# ==========================================\n",
    "class SoftTargetKLLoss(nn.Module):\n",
    "    \"\"\"Exp 3a: KL Divergence mot statiske sannsynlighetsmål.\"\"\"\n",
    "    def __init__(self, targets_probs, ce_anchor=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"T\", targets_probs)\n",
    "        self.ce_anchor = ce_anchor\n",
    "        self.kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        # targets: (B, C)\n",
    "        targets = self.T[y] \n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = self.kl(log_probs, targets)\n",
    "        if self.ce_anchor > 0:\n",
    "            loss += self.ce_anchor * self.ce(logits, y)\n",
    "        return loss\n",
    "\n",
    "class LogitTargetWMSENLoss(nn.Module):\n",
    "    \"\"\"Exp 3b/4/5: Weighted MSE mot logit-mål.\"\"\"\n",
    "    def __init__(self, ce_anchor=0.0, logit_l2=0.0, true_class_boost=1.0):\n",
    "        super().__init__()\n",
    "        self.ce_anchor = ce_anchor\n",
    "        self.logit_l2 = logit_l2\n",
    "        self.true_class_boost = true_class_boost\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, y, current_targets, current_weights):\n",
    "        # current_targets: (C, C)\n",
    "        # current_weights: (C)\n",
    "        \n",
    "        batch_targets = current_targets[y] # (B, C)\n",
    "        diff = (logits - batch_targets) ** 2\n",
    "        \n",
    "        B, C = logits.shape\n",
    "        w_batch = current_weights.view(1, -1).expand(B, -1).clone()\n",
    "        \n",
    "        # Boost true class weight\n",
    "        if self.true_class_boost != 1.0:\n",
    "            idx = torch.arange(B, device=logits.device)\n",
    "            w_batch[idx, y] *= self.true_class_boost\n",
    "            \n",
    "        weighted_mse = (diff * w_batch).mean()\n",
    "        loss = weighted_mse\n",
    "        \n",
    "        if self.ce_anchor > 0:\n",
    "            loss += self.ce_anchor * self.ce(logits, y)\n",
    "        if self.logit_l2 > 0:\n",
    "            loss += self.logit_l2 * (logits ** 2).mean()\n",
    "            \n",
    "        return loss\n",
    "\n",
    "# ==========================================\n",
    "# 5. UNIFIED TRAINER CLASS\n",
    "# ==========================================\n",
    "class UnifiedTrainer:\n",
    "    def __init__(self, config, run_name):\n",
    "        self.cfg = config\n",
    "        self.run_name = run_name\n",
    "        \n",
    "        # Paths\n",
    "        self.artifact_dir = Path(f\"./artifacts/{run_name}\")\n",
    "        self.ckpt_dir = Path(\"./checkpoints\")\n",
    "        self.artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup Data\n",
    "        self.setup_data()\n",
    "        \n",
    "        # Setup Model\n",
    "        self.model = make_cifar_resnet18(NUM_CLASSES).to(DEVICE)\n",
    "        \n",
    "        # Optimizer & Scaler\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.cfg['lr'], weight_decay=1e-4)\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.cfg['use_amp'])\n",
    "        \n",
    "        # Loss Init\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Experiment Specific State\n",
    "        self.sbls_matrix = None\n",
    "        self.targets_matrix = None # Kan være probs (3a) eller logits (3b/4/5)\n",
    "        self.targets_matrix_0 = None # Anker for dynamisk\n",
    "        self.weights = torch.ones(NUM_CLASSES, device=DEVICE) # For WMSE\n",
    "        \n",
    "        # History\n",
    "        self.history = {\"loss\": [], \"train_acc\": [], \"test_acc\": [], \"epoch_times\": []}\n",
    "        \n",
    "        self._init_experiment_logic()\n",
    "\n",
    "    def setup_data(self):\n",
    "        train_tf = T.Compose([\n",
    "            T.RandomCrop(32, padding=4), T.RandomHorizontalFlip(), T.ToTensor(),\n",
    "            T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "        eval_tf = T.Compose([\n",
    "            T.ToTensor(), T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "        \n",
    "        # Dataset\n",
    "        train_ds_aug = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "        train_ds_eval = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=eval_tf)\n",
    "        test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=eval_tf)\n",
    "        \n",
    "        self.train_loader = DataLoader(train_ds_aug, batch_size=self.cfg['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.train_eval_loader = DataLoader(train_ds_eval, batch_size=self.cfg['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "        self.test_loader = DataLoader(test_ds, batch_size=self.cfg['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    def _init_experiment_logic(self):\n",
    "        \"\"\"Laster targets eller initierer matriser basert på mode.\"\"\"\n",
    "        mode = self.cfg['mode']\n",
    "        \n",
    "        if mode == 'sbls':\n",
    "            self.sbls_matrix = build_similarity_matrix_cifar10().to(DEVICE)\n",
    "            \n",
    "        elif mode == 'kl': # Exp 3a\n",
    "            if 'targets_path' in self.cfg:\n",
    "                d = torch.load(self.cfg['targets_path'], map_location=DEVICE)\n",
    "                self.targets_matrix = d['class_avg_probs']\n",
    "                self.criterion_kl = SoftTargetKLLoss(self.targets_matrix, ce_anchor=self.cfg.get('ce_anchor', 0))\n",
    "            else:\n",
    "                raise ValueError(\"Mode 'kl' requires 'targets_path'\")\n",
    "                \n",
    "        elif mode in ['wmse_static', 'wmse_dynamic']: # Exp 3b, 4, 5\n",
    "            if 'targets_path' in self.cfg:\n",
    "                d = torch.load(self.cfg['targets_path'], map_location=DEVICE)\n",
    "                self.targets_matrix = d['class_avg_logits']\n",
    "                \n",
    "                # Hvis vi skal bruke cap/swap initialt (Exp 5 starttilstand)\n",
    "                if self.cfg.get('swap_enabled', False):\n",
    "                    self.targets_matrix, _ = enforce_target_dominance(self.targets_matrix)\n",
    "                if self.cfg.get('cap_enabled', False):\n",
    "                    self.targets_matrix = apply_probability_cap(self.targets_matrix, max_p=self.cfg['max_prob'])\n",
    "                \n",
    "                self.targets_matrix_0 = self.targets_matrix.clone() # Anker\n",
    "                \n",
    "                self.criterion_wmse = LogitTargetWMSENLoss(\n",
    "                    ce_anchor=self.cfg.get('ce_anchor', 0.05),\n",
    "                    logit_l2=self.cfg.get('logit_l2', 0.0),\n",
    "                    true_class_boost=self.cfg.get('true_class_boost', 1.0)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"WMSE modes require 'targets_path'\")\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        \n",
    "        for x, y in self.train_loader:\n",
    "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=self.cfg['use_amp']):\n",
    "                logits = self.model(x)\n",
    "                \n",
    "                # --- VELG LOSS BASERT PÅ MODE ---\n",
    "                if self.cfg['mode'] == 'baseline':\n",
    "                    loss = self.ce_loss(logits, y)\n",
    "                    \n",
    "                elif self.cfg['mode'] == 'sbls':\n",
    "                    soft_T = make_soft_targets_sbls(y, self.sbls_matrix, alpha=self.cfg['sbls_alpha'])\n",
    "                    log_probs = F.log_softmax(logits, dim=1)\n",
    "                    loss = -(soft_T * log_probs).sum(dim=1).mean()\n",
    "                    \n",
    "                elif self.cfg['mode'] == 'kl':\n",
    "                    loss = self.criterion_kl(logits, y)\n",
    "                    \n",
    "                elif self.cfg['mode'] in ['wmse_static', 'wmse_dynamic']:\n",
    "                    loss = self.criterion_wmse(logits, y, self.targets_matrix, self.weights)\n",
    "            \n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            if self.cfg.get('grad_clip', 0) > 0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg['grad_clip'])\n",
    "                \n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "            \n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits = self.model(x)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_per_class_accuracy(self):\n",
    "        self.model.eval()\n",
    "        correct = torch.zeros(NUM_CLASSES, device=DEVICE)\n",
    "        counts = torch.zeros(NUM_CLASSES, device=DEVICE)\n",
    "        for x, y in self.train_eval_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = self.model(x).argmax(1)\n",
    "            for c in range(NUM_CLASSES):\n",
    "                mask = (y == c)\n",
    "                if mask.any():\n",
    "                    counts[c] += mask.sum()\n",
    "                    correct[c] += (pred[mask] == c).sum()\n",
    "        return correct / counts.clamp_min(1)\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"Starter trening: {self.run_name} ({self.cfg['epochs']} epoker)\")\n",
    "        print(f\"Mode: {self.cfg['mode']}\")\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for ep in range(1, self.cfg['epochs'] + 1):\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # --- PRE-EPOCH DYNAMICS (Exp 4/5) ---\n",
    "            if self.cfg['mode'] == 'wmse_dynamic':\n",
    "                if ep == 1:\n",
    "                    current_acc = torch.ones(NUM_CLASSES, device=DEVICE) * 0.5\n",
    "                else:\n",
    "                    current_acc = self.get_per_class_accuracy()\n",
    "                    \n",
    "                    # 1. Update based on acc\n",
    "                    self.targets_matrix, changes = update_target_logits_dynamic(\n",
    "                        self.targets_matrix, current_acc, \n",
    "                        T0=self.targets_matrix_0,\n",
    "                        beta=self.cfg.get('auth_beta', 10.0),\n",
    "                        lr=self.cfg.get('target_update_lr', 0.1),\n",
    "                        kappa=self.cfg.get('kappa', 0.1),\n",
    "                        anchor_gamma=self.cfg.get('anchor_gamma', 0.0),\n",
    "                        clamp_M=self.cfg.get('clamp_m', 10.0)\n",
    "                    )\n",
    "                    \n",
    "                    # 2. Swap (Optional)\n",
    "                    if self.cfg.get('swap_enabled', False):\n",
    "                        self.targets_matrix, swaps = enforce_target_dominance(self.targets_matrix)\n",
    "                        \n",
    "                    # 3. Cap (Optional)\n",
    "                    if self.cfg.get('cap_enabled', False):\n",
    "                        self.targets_matrix = apply_probability_cap(self.targets_matrix, max_p=self.cfg['max_prob'])\n",
    "                \n",
    "                # Oppdater vekter for WMSE\n",
    "                self.weights = weights_from_acc(current_acc, gamma=self.cfg.get('weight_gamma', 1.0))\n",
    "\n",
    "            # --- TRAIN ---\n",
    "            tr_loss, tr_acc = self.train_epoch(ep)\n",
    "            \n",
    "            # --- EVAL ---\n",
    "            te_acc = self.evaluate(self.test_loader)\n",
    "            dt = time.time() - t0\n",
    "            \n",
    "            self.history[\"loss\"].append(tr_loss)\n",
    "            self.history[\"train_acc\"].append(tr_acc)\n",
    "            self.history[\"test_acc\"].append(te_acc)\n",
    "            self.history[\"epoch_times\"].append(dt)\n",
    "            \n",
    "            print(f\"Ep {ep:03d} | Loss: {tr_loss:.4f} | TrAcc: {tr_acc:.3f} | TeAcc: {te_acc:.3f} | T: {dt:.1f}s\")\n",
    "            \n",
    "            # Lagre best\n",
    "            if te_acc > best_acc:\n",
    "                best_acc = te_acc\n",
    "                torch.save(self.model.state_dict(), self.ckpt_dir / f\"{self.run_name}_best.pth\")\n",
    "                \n",
    "        # Lagre sluttresultater\n",
    "        self._save_results()\n",
    "        print(f\"Ferdig! Best Test Acc: {best_acc:.4f}\")\n",
    "        return best_acc\n",
    "\n",
    "    def _save_results(self):\n",
    "        # Lagre history json\n",
    "        with open(self.artifact_dir / \"history.json\", \"w\") as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "            \n",
    "        # Lagre targets matrise hvis relevant\n",
    "        if self.targets_matrix is not None:\n",
    "            torch.save(self.targets_matrix, self.artifact_dir / \"final_targets.pt\")\n",
    "            # Lag også en heatmap hvis mulig (lagres som pt, plotting skjer i utils)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
