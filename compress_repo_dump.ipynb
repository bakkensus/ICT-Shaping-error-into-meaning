{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bc42bd-6ad1-40a4-be21-feb57675050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanner 'artifacts' (rekursivt) med distribuert sampling...\n",
      "  -> Behandlet: training_stats-checkpoint.json\n",
      "  -> Behandlet: training_stats.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results-checkpoint.json\n",
      "  -> Behandlet: weights_history-checkpoint.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: weights_history.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: weights_history.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results-checkpoint.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: results.json\n",
      "  -> Behandlet: analysis_fair-checkpoint.csv\n",
      "  -> Behandlet: analysis_full_comparison-checkpoint.csv\n",
      "  -> Behandlet: analysis_general-checkpoint.csv\n",
      "  -> Behandlet: analysis_fair.csv\n",
      "  -> Behandlet: analysis_full_comparison.csv\n",
      "  -> Behandlet: analysis_general.csv\n",
      "  -> Behandlet: weighted_human_accuracy.csv\n",
      "  -> Behandlet: weighted_human_accuracy_9p.csv\n",
      "\n",
      "Ferdig! 38 filer lagret til 'repo_data_dump.txt'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASJON\n",
    "# ==========================================\n",
    "TARGET_DIR = Path(\"./artifacts\")        # Mappen vi skal skanne\n",
    "OUTPUT_FILENAME = \"repo_data_dump.txt\"  # Resultatfilen\n",
    "\n",
    "ALLOWED_EXTENSIONS = {'.json', '.csv'}\n",
    "\n",
    "# --- SAMPLING INNSTILLINGER ---\n",
    "# Dette styrer \"S-kurve\" logikken.\n",
    "TARGET_TOTAL_ITEMS = 30   # Hvor mange elementer vi totalt vil vise fra en lang liste\n",
    "WINDOW_SIZE = 3           # Hvor mange elementer vi viser sammenhengende i hver \"chunk\"\n",
    "MAX_JSON_DEPTH = 4        # Hvor dypt ned i JSON-strukturen vi går\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOGIKK FOR DISTRIBUERT SAMPLING\n",
    "# ==========================================\n",
    "\n",
    "def get_sampled_list(full_list):\n",
    "    \"\"\"\n",
    "    Tar en lang liste og returnerer en samplet versjon som viser data\n",
    "    jevnt fordelt utover hele listen (start, midt, slutt osv).\n",
    "    \"\"\"\n",
    "    total_len = len(full_list)\n",
    "    \n",
    "    # Hvis listen er kort nok, vis alt\n",
    "    if total_len <= TARGET_TOTAL_ITEMS:\n",
    "        return full_list\n",
    "    \n",
    "    sampled_output = []\n",
    "    \n",
    "    # Beregn hvor mange \"vinduer\" (grupper) vi har plass til\n",
    "    num_windows = max(2, TARGET_TOTAL_ITEMS // WINDOW_SIZE)\n",
    "    \n",
    "    # Beregn steget (hvor langt vi hopper mellom hvert vindu)\n",
    "    step = (total_len - WINDOW_SIZE) / (num_windows - 1)\n",
    "    \n",
    "    last_idx = -1\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        # Finn start-indeksen for dette vinduet\n",
    "        start_idx = int(i * step)\n",
    "        \n",
    "        # Unngå overlapp eller å gå bakover (sikkerhetsnett)\n",
    "        if start_idx <= last_idx:\n",
    "            start_idx = last_idx + 1\n",
    "        \n",
    "        end_idx = min(start_idx + WINDOW_SIZE, total_len)\n",
    "        \n",
    "        # Hvis vi hoppet over data, legg til en indikator\n",
    "        if start_idx > last_idx + 1 and last_idx != -1:\n",
    "            skipped_count = start_idx - (last_idx + 1)\n",
    "            sampled_output.append(f\"... [skipped {skipped_count} items] ...\")\n",
    "        \n",
    "        # Legg til selve dataene i vinduet\n",
    "        chunk = full_list[start_idx:end_idx]\n",
    "        sampled_output.extend(chunk)\n",
    "        \n",
    "        last_idx = end_idx - 1\n",
    "        \n",
    "        # Hvis vi har nådd slutten, stopp\n",
    "        if last_idx >= total_len - 1:\n",
    "            break\n",
    "            \n",
    "    return sampled_output\n",
    "\n",
    "def compress_json_recursive(data, depth=0):\n",
    "    \"\"\"\n",
    "    Rekursiv JSON-behandling som bruker sampling på lister.\n",
    "    \"\"\"\n",
    "    if depth >= MAX_JSON_DEPTH:\n",
    "        return \"...\" \n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        new_dict = {}\n",
    "        keys = list(data.keys())\n",
    "        \n",
    "        # For dictionaries viser vi vanligvis alle nøkler med mindre det er ekstremt mange,\n",
    "        # fordi nøkler ofte er unike identifikatorer.\n",
    "        # Men vi sampler innholdet i verdiene.\n",
    "        limit_keys = 20 # Sikkerhetsgrense for dicts med tusenvis av nøkler\n",
    "        \n",
    "        for i, k in enumerate(keys):\n",
    "            if i >= limit_keys:\n",
    "                new_dict[f\"...\"] = f\"({len(keys) - limit_keys} more keys hidden)\"\n",
    "                break\n",
    "            new_dict[k] = compress_json_recursive(data[k], depth + 1)\n",
    "        return new_dict\n",
    "    \n",
    "    elif isinstance(data, list):\n",
    "        # Her skjer magien: Vi sampler listen i stedet for å kutte den tvert\n",
    "        # Vi rekursivt komprimerer elementene vi beholder også\n",
    "        sampled_raw = get_sampled_list(data)\n",
    "        \n",
    "        # Kjør rekursjon på elementene vi beholdt (hvis de er lister/dicts)\n",
    "        processed_list = []\n",
    "        for item in sampled_raw:\n",
    "            if isinstance(item, str) and item.startswith(\"... [skipped\"):\n",
    "                processed_list.append(item) # Behold skip-markøren\n",
    "            else:\n",
    "                processed_list.append(compress_json_recursive(item, depth + 1))\n",
    "                \n",
    "        return processed_list\n",
    "    \n",
    "    return data # Returner primitive typer som de er\n",
    "\n",
    "def process_csv_content(text_content):\n",
    "    \"\"\"\n",
    "    Sampler CSV-linjer fordelt utover filen.\n",
    "    \"\"\"\n",
    "    lines = text_content.splitlines()\n",
    "    if not lines: return \"\"\n",
    "    \n",
    "    header = lines[0]\n",
    "    data_rows = lines[1:]\n",
    "    \n",
    "    if not data_rows: return header\n",
    "    \n",
    "    # Bruk samme sampling-logikk på radene\n",
    "    sampled_rows = get_sampled_list(data_rows)\n",
    "    \n",
    "    output = [header]\n",
    "    for row in sampled_rows:\n",
    "        output.append(str(row)) # row er en streng eller skip-markør\n",
    "        \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# ==========================================\n",
    "# 3. FILBEHANDLING OG MAIN\n",
    "# ==========================================\n",
    "\n",
    "def get_directory_tree(root_path):\n",
    "    \"\"\"Lager en enkel tekst-trestruktur av filene vi fant.\"\"\"\n",
    "    lines = [f\"=== ARTIFACTS STRUCTURE ({root_path}) ===\"]\n",
    "    all_files = sorted([p for p in root_path.rglob(\"*\") if p.is_file() and p.suffix.lower() in ALLOWED_EXTENSIONS])\n",
    "    \n",
    "    if not all_files:\n",
    "        return \"No .json or .csv files found in artifacts folder.\\n\"\n",
    "\n",
    "    for path in all_files:\n",
    "        rel_path = path.relative_to(root_path.parent)\n",
    "        lines.append(f\"- {rel_path}\")\n",
    "    lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def process_file(file_path):\n",
    "    header = f\"\\n{'='*80}\\nFILE: {file_path}\\n{'='*80}\\n\"\n",
    "    try:\n",
    "        if file_path.suffix.lower() == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            compressed = compress_json_recursive(data)\n",
    "            return header + json.dumps(compressed, indent=2) + \"\\n\"\n",
    "            \n",
    "        elif file_path.suffix.lower() == '.csv':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            processed = process_csv_content(content)\n",
    "            return header + processed + \"\\n\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return header + f\"[ERROR: {e}]\\n\"\n",
    "    return \"\"\n",
    "\n",
    "def main():\n",
    "    if not TARGET_DIR.exists():\n",
    "        print(f\"Feil: Fant ikke mappen '{TARGET_DIR}'.\")\n",
    "        return\n",
    "\n",
    "    full_output = []\n",
    "    print(f\"Scanner '{TARGET_DIR}' (rekursivt) med distribuert sampling...\")\n",
    "    \n",
    "    # 1. Struktur\n",
    "    full_output.append(get_directory_tree(TARGET_DIR))\n",
    "    \n",
    "    # 2. Innhold\n",
    "    count = 0\n",
    "    for file_path in sorted(TARGET_DIR.rglob(\"*\")):\n",
    "        if file_path.is_file() and file_path.suffix.lower() in ALLOWED_EXTENSIONS:\n",
    "            full_output.append(process_file(file_path))\n",
    "            count += 1\n",
    "            print(f\"  -> Behandlet: {file_path.name}\")\n",
    "\n",
    "    # 3. Lagre\n",
    "    with open(OUTPUT_FILENAME, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\".join(full_output))\n",
    "        \n",
    "    print(f\"\\nFerdig! {count} filer lagret til '{OUTPUT_FILENAME}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cc01f-0724-4d1f-85f4-78308f8e3014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
