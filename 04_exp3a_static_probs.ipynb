{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3933f96-a675-49b5-968e-702ebd7db46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1 (Imports) ===\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === CELL 2 (Configuration) ===\n",
    "# --- Konfigurasjon ---\n",
    "EXPERIMENT_NAME = \"04_exp3a_static_probs\"\n",
    "DATA_DIR = Path(\"./data\")\n",
    "BASE_ARTIFACTS_DIR = Path(f\"./artifacts/{EXPERIMENT_NAME}\")\n",
    "CKPT_DIR = Path(\"./checkpoints\")\n",
    "\n",
    "# Sti til Teacher Targets (fra 01_setup_probe)\n",
    "TARGETS_PATH = Path(\"./artifacts/01_probe_baseline/probe_targets.pt\")\n",
    "\n",
    "# Opprett mapper\n",
    "BASE_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparametere\n",
    "RUNS = [50, 300]  # Vi skal kjøre to varianter: 50 epoker og 300 epoker\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "USE_AMP = True\n",
    "\n",
    "# Exp 3a Spesifikke parametere (fra gammel kode)\n",
    "KL_EPS = 1e-8           # Gulv for numerisk stabilitet\n",
    "ONEHOT_MIX_EPS = 0.10   # Hvor mye \"hard\" one-hot vi blander inn i soft-targets (0.10 = 10%)\n",
    "CE_ANCHOR = 0.0         # Lite anker mot standard CE loss (0.0 = kun KL)\n",
    "\n",
    "# Data & Klasser\n",
    "NUM_CLASSES = 10\n",
    "CIFAR10_CLASSES = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "print(f\"Konfigurasjon satt for {EXPERIMENT_NAME}.\")\n",
    "print(f\"Artefakter lagres til: {BASE_ARTIFACTS_DIR}\")\n",
    "\n",
    "if not TARGETS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Fant ikke teacher targets på {TARGETS_PATH}. Har du kjørt 01_setup_probe.ipynb?\")\n",
    "\n",
    "# === CELL 3 (Reproducibility) ===\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kjører på: {device}\")\n",
    "\n",
    "# === CELL 4 (Data Loading) ===\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "eval_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_ds_aug = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_tf)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=eval_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds_aug, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# === CELL 5 (Model Definition) ===\n",
    "def make_cifar_resnet18(num_classes=10):\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "# === CELL 6 (Custom Loss Function - KL Divergence) ===\n",
    "class ProbabilityTargetLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    KLDivLoss mellom lærte klasse-gjennomsnittlige soft targets (T) og modellens prediksjoner (P).\n",
    "    Inkluderer mulighet for å blande inn one-hot (mix_eps) og et CE-anker.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_avg_probs: torch.Tensor, onehot_mix_eps: float = 0.0, ce_anchor: float = 0.0):\n",
    "        super().__init__()\n",
    "        # T er en matrise [NumClasses, NumClasses] hvor rad 'c' er target-fordelingen for klasse 'c'.\n",
    "        self.register_buffer(\"T\", class_avg_probs)\n",
    "        self.onehot_mix_eps = float(onehot_mix_eps)\n",
    "        self.ce_anchor = float(ce_anchor)\n",
    "        self.kldiv = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        # 1. Hent targets basert på sann klasse y\n",
    "        targets = self.T[y]  # (B, C)\n",
    "\n",
    "        # 2. Bland inn One-Hot hvis ønsket (Stabilisering)\n",
    "        if self.onehot_mix_eps > 0.0:\n",
    "            one_hot = torch.zeros_like(targets)\n",
    "            one_hot.scatter_(1, y.view(-1, 1), 1.0)\n",
    "            targets = (1.0 - self.onehot_mix_eps) * targets + self.onehot_mix_eps * one_hot\n",
    "            \n",
    "            # Normaliser og clamp\n",
    "            targets = torch.clamp(targets, min=KL_EPS)\n",
    "            targets = targets / targets.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # 3. Beregn KL Divergens\n",
    "        # PyTorch KLDiv krever log-probabilities som input, og probabilities som target\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = self.kldiv(log_probs, targets)\n",
    "\n",
    "        # 4. Legg til CE Anker hvis ønsket\n",
    "        if self.ce_anchor > 0.0:\n",
    "            loss += self.ce_anchor * self.ce(logits, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# === CELL 7 (Load & Visualize Targets) ===\n",
    "# Last inn data generert av Sonde-modellen\n",
    "payload = torch.load(TARGETS_PATH, map_location=device)\n",
    "class_avg_probs = payload[\"class_avg_probs\"] # Dette er soft targets\n",
    "\n",
    "print(f\"Lastet targets med form: {class_avg_probs.shape}\")\n",
    "\n",
    "# Visualiser hva modellen skal prøve å lære (Target Heatmap)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    class_avg_probs.cpu().numpy(),\n",
    "    xticklabels=CIFAR10_CLASSES,\n",
    "    yticklabels=CIFAR10_CLASSES,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"viridis\",\n",
    "    cbar_kws={'label': 'Target Probability'}\n",
    ")\n",
    "plt.title(f\"Experiment 3a Targets: Soft Probabilities (Probe-Learned)\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.tight_layout()\n",
    "\n",
    "heatmap_path = BASE_ARTIFACTS_DIR / \"target_distribution_heatmap.png\"\n",
    "plt.savefig(heatmap_path)\n",
    "print(f\"Heatmap lagret til: {heatmap_path}\")\n",
    "plt.show()\n",
    "\n",
    "# === CELL 8 (Training & Eval Functions) ===\n",
    "def train_one_epoch(model, loader, optimizer, scaler, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\" if device.type == \"cuda\" else \"cpu\", enabled=USE_AMP):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct_top1 = 0\n",
    "    correct_top2 = 0\n",
    "    ranks_sum = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Top-k metrics\n",
    "        top2 = probs.topk(2, dim=1).indices\n",
    "        pred_top1 = top2[:, 0]\n",
    "        correct_top1 += (pred_top1 == y).sum().item()\n",
    "        correct_top2 += ((top2[:, 0] == y) | (top2[:, 1] == y)).sum().item()\n",
    "        \n",
    "        # Mean Rank\n",
    "        sorted_indices = logits.argsort(dim=1, descending=True)\n",
    "        ranks = (sorted_indices == y.view(-1, 1)).nonzero()[:, 1] + 1\n",
    "        ranks_sum += ranks.float().sum().item()\n",
    "        \n",
    "        total += x.size(0)\n",
    "        \n",
    "    return {\n",
    "        \"acc1\": correct_top1 / total,\n",
    "        \"acc2\": correct_top2 / total,\n",
    "        \"mean_rank\": ranks_sum / total\n",
    "    }\n",
    "\n",
    "# === CELL 9 (Main Experiment Runner) ===\n",
    "# Vi kjører loopen for både 50 og 300 epoker som forespurt\n",
    "for max_epochs in RUNS:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"  STARTER KJØRING: {max_epochs} EPOKER\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Initier for denne kjøringen\n",
    "    run_dir = BASE_ARTIFACTS_DIR / f\"run_{max_epochs}ep\"\n",
    "    run_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model = make_cifar_resnet18(NUM_CLASSES).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    # Sett opp KL-tapsfunksjonen med våre loaded targets\n",
    "    criterion = ProbabilityTargetLoss(\n",
    "        class_avg_probs=class_avg_probs, \n",
    "        onehot_mix_eps=ONEHOT_MIX_EPS, \n",
    "        ce_anchor=CE_ANCHOR\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"test_acc1\": [], \"test_acc2\": [], \"mean_rank\": []}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        t_loss, t_acc = train_one_epoch(model, train_loader, optimizer, scaler, criterion)\n",
    "        metrics = evaluate(model, test_loader)\n",
    "        \n",
    "        history[\"train_loss\"].append(t_loss)\n",
    "        history[\"train_acc\"].append(t_acc)\n",
    "        history[\"test_acc1\"].append(metrics[\"acc1\"])\n",
    "        history[\"test_acc2\"].append(metrics[\"acc2\"])\n",
    "        history[\"mean_rank\"].append(metrics[\"mean_rank\"])\n",
    "        \n",
    "        if metrics[\"acc1\"] > best_acc:\n",
    "            best_acc = metrics[\"acc1\"]\n",
    "            torch.save(model.state_dict(), CKPT_DIR / f\"{EXPERIMENT_NAME}_{max_epochs}ep_best.pth\")\n",
    "            \n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Ep {epoch:03d} | Loss: {t_loss:.4f} | TrAcc: {t_acc:.3f} | \"\n",
    "                  f\"TeAcc1: {metrics['acc1']:.3f} | Rank: {metrics['mean_rank']:.2f} | T: {elapsed:.0f}s\")\n",
    "            \n",
    "    # Lagre resultater for denne kjøringen\n",
    "    res_file = run_dir / \"results.json\"\n",
    "    with open(res_file, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"config\": {\n",
    "                \"epochs\": max_epochs,\n",
    "                \"onehot_mix_eps\": ONEHOT_MIX_EPS,\n",
    "                \"ce_anchor\": CE_ANCHOR\n",
    "            },\n",
    "            \"best_acc\": best_acc,\n",
    "            \"history\": history\n",
    "        }, f, indent=2)\n",
    "        \n",
    "    # Lagre plott av trening\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.title(f\"Training Loss ({max_epochs} epochs)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(run_dir / \"loss_curve.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Ferdig med {max_epochs} epoker. Beste Acc: {best_acc:.4f}. Data lagret i {run_dir}\")\n",
    "\n",
    "print(f\"\\nAlle kjøringer fullført for {EXPERIMENT_NAME}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
