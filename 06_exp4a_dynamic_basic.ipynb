{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369632ff-e983-4c54-bc0a-134776168d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1 (Complete Experiment 4a Code) ===\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASJON & OPPSETT\n",
    "# ==========================================\n",
    "EXPERIMENT_NAME = \"06_exp4a_dynamic_basic\"\n",
    "DATA_DIR = Path(\"./data\")\n",
    "BASE_ARTIFACTS_DIR = Path(f\"./artifacts/{EXPERIMENT_NAME}\")\n",
    "CKPT_DIR = Path(\"./checkpoints\")\n",
    "\n",
    "# Startpunkt: Vi bruker Logits lært av Proben (Eksperiment 1 / 01_setup_probe)\n",
    "# Dette tilsvarer logikken i eks4_2 hvor man lastet \"3b\" targets som start.\n",
    "TARGETS_PATH = Path(\"./artifacts/01_probe_baseline/probe_targets.pt\")\n",
    "\n",
    "# Opprett mapper\n",
    "BASE_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparametere (Generelt)\n",
    "RUNS = [50, 300]        # Kjører både 50 og 300 epoker\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "USE_AMP = True\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "# Hyperparametere (Fra eks4_2.ipynb - Dynamic Logic)\n",
    "MIN_ACC_DIFF = 0.05         # Minimum forskjell i presisjon for at dynamikken skal slå inn\n",
    "AUTH_BETA = 10.0            # Sigmoid-skarphet for autoritet\n",
    "TARGET_UPDATE_LR = 0.10     # Steglengde for target-oppdatering (eta)\n",
    "KAPPA_PUSHBACK = 0.10       # \"Push\" på eksperten (repulsjon)\n",
    "TARGET_ANCHOR_GAMMA = 0.01  # Anker mot probe-targets per epoke (hindrer total drift)\n",
    "TARGET_CLAMP_M = 10.0       # Clamp targets til [-M, M]\n",
    "\n",
    "# Hyperparametere (Vekting fra 3b som brukes i 4a)\n",
    "WEIGHT_GAMMA = 1.0\n",
    "WEIGHT_EPS = 1e-6\n",
    "CE_ANCHOR = 0.05            # Lite anker for stabilitet\n",
    "LOGIT_L2 = 0.001            # L2 på logits for å hindre eksplosjon\n",
    "\n",
    "# Data & Klasser\n",
    "NUM_CLASSES = 10\n",
    "CIFAR10_CLASSES = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "print(f\"Konfigurasjon satt for {EXPERIMENT_NAME}.\")\n",
    "print(f\"Artefakter lagres til: {BASE_ARTIFACTS_DIR}\")\n",
    "\n",
    "if not TARGETS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Fant ikke teacher targets på {TARGETS_PATH}. Har du kjørt 01_setup_probe.ipynb?\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. REPRODUSERBARHET\n",
    "# ==========================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kjører på: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "eval_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_ds_aug = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_tf)\n",
    "# Vi trenger eval-loader for training set for å måle presisjon (\"Autoritet\") før hver epoke\n",
    "train_ds_eval = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=eval_tf)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=eval_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds_aug, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "train_eval_loader = DataLoader(train_ds_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODELL DEFINISJON\n",
    "# ==========================================\n",
    "def make_cifar_resnet18(num_classes=10):\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "# ==========================================\n",
    "# 5. KJERNE-LOGIKK FOR EKS 4 (Dynamisk Oppdatering)\n",
    "# ==========================================\n",
    "# Kopiert og tilpasset fra eks4_2.ipynb\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_target_logits(\n",
    "    T,\n",
    "    acc,\n",
    "    T0=None,\n",
    "    beta=10.0,\n",
    "    lr=0.10,\n",
    "    kappa=0.10,\n",
    "    min_diff=0.05,\n",
    "    anchor_gamma=0.01,\n",
    "    clamp_M=10.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Oppdaterer mål-matrisen T basert på asymmetrisk autoritet (sigmoid).\n",
    "    - Novise (i) flyttes mot ekspertens syn T[j,i] på relasjonen.\n",
    "    - (Valgfritt) Ekspert (j) skyves svakt vekk (repulsjon) via kappa.\n",
    "    - (Valgfritt) Anker mot probe-targets T0 via anchor_gamma.\n",
    "    - (Valgfritt) Clamp for å hindre drift/eksplosjon.\n",
    "    \"\"\"\n",
    "    C = T.shape[0]\n",
    "    T_new = T.clone()\n",
    "    updates_made = 0\n",
    "\n",
    "    # Iterer alle par\n",
    "    for i in range(C):          # i = novise\n",
    "        for j in range(C):      # j = ekspert\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            diff = acc[j] - acc[i]\n",
    "            if diff <= min_diff:\n",
    "                continue\n",
    "\n",
    "            # Autoritet: p in (0,1)\n",
    "            p = torch.sigmoid(beta * diff)\n",
    "\n",
    "            # Ekspertens syn på relasjonen: T[j, i]\n",
    "            expert_view = T[j, i]\n",
    "            novice_view = T[i, j]\n",
    "\n",
    "            # Trekk novisen mot eksperten\n",
    "            change = lr * p * (expert_view - novice_view)\n",
    "            T_new[i, j] += change\n",
    "\n",
    "            # (Valgfritt) pushback: skyv eksperten svakt vekk fra novisen (repulsjon)\n",
    "            if kappa > 0:\n",
    "                T_new[j, i] -= kappa * change\n",
    "\n",
    "            updates_made += 1\n",
    "\n",
    "    # (Valgfritt) Anker mot probe-targets (hindrer total drift bort fra læreren)\n",
    "    if T0 is not None and anchor_gamma > 0:\n",
    "        T_new = (1.0 - anchor_gamma) * T_new + anchor_gamma * T0\n",
    "\n",
    "    # (Valgfritt) Clamp for stabilitet\n",
    "    if clamp_M is not None and clamp_M > 0:\n",
    "        T_new = T_new.clamp(min=-clamp_M, max=clamp_M)\n",
    "\n",
    "    return T_new, updates_made\n",
    "\n",
    "# ==========================================\n",
    "# 6. CUSTOM LOSS & HJELPEFUNKSJONER\n",
    "# ==========================================\n",
    "\n",
    "class DynamicTargetWMSE(nn.Module):\n",
    "    def __init__(self, ce_anchor=0.0, logit_l2=0.0):\n",
    "        super().__init__()\n",
    "        self.ce_anchor = ce_anchor\n",
    "        self.logit_l2 = logit_l2\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, y, current_targets, current_weights):\n",
    "        # current_targets: (C, C) matrix used for lookup\n",
    "        # current_weights: (C) vector\n",
    "        \n",
    "        # 1. Hent targets for batchen\n",
    "        batch_targets = current_targets[y] # (B, C)\n",
    "        \n",
    "        # 2. WMSE Loss\n",
    "        # (Logits - Targets)^2 * Weight\n",
    "        diff = (logits - batch_targets) ** 2\n",
    "        \n",
    "        # Expand weights to batch: (1, C) -> (B, C) (broadcasting)\n",
    "        w_expanded = current_weights.view(1, -1)\n",
    "        \n",
    "        # Apply weights: Vi straffer feil hardere på dimensjoner (kolonner) \n",
    "        # som modellen er god på (høy vekt).\n",
    "        weighted_mse = (diff * w_expanded).mean()\n",
    "        loss = weighted_mse\n",
    "        \n",
    "        # 3. Stabilisering\n",
    "        if self.ce_anchor > 0:\n",
    "            loss += self.ce_anchor * self.ce(logits, y)\n",
    "            \n",
    "        if self.logit_l2 > 0:\n",
    "            loss += self.logit_l2 * (logits ** 2).mean()\n",
    "            \n",
    "        return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def per_class_accuracy(model, loader, num_classes=10):\n",
    "    model.eval()\n",
    "    correct = torch.zeros(num_classes, device=device)\n",
    "    counts = torch.zeros(num_classes, device=device)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        pred = model(x).argmax(dim=1)\n",
    "        for c in range(num_classes):\n",
    "            mask = (y == c)\n",
    "            if mask.any():\n",
    "                counts[c] += mask.sum()\n",
    "                correct[c] += (pred[mask] == c).sum()\n",
    "    # Unngå divisjon på null\n",
    "    acc = correct / counts.clamp_min(1)\n",
    "    return acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def weights_from_acc(acc, gamma=1.0, eps=1e-6):\n",
    "    # Higher accuracy -> Higher weight\n",
    "    w = (acc.clamp_min(0.0) + eps).pow(gamma)\n",
    "    w = w / w.mean().clamp_min(1e-8) # Normalize\n",
    "    return w\n",
    "\n",
    "def save_target_heatmap(logits_matrix, classes, filename, title_suffix=\"\"):\n",
    "    \"\"\"Lager og lagrer en heatmap av mål-matrisen (konvertert til sannsynligheter).\"\"\"\n",
    "    probs = torch.softmax(logits_matrix, dim=1).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        probs, \n",
    "        xticklabels=classes, \n",
    "        yticklabels=classes, \n",
    "        annot=True, \n",
    "        fmt=\".2f\", \n",
    "        cmap=\"viridis\",\n",
    "        cbar_kws={'label': 'Target Probability'}\n",
    "    )\n",
    "    plt.title(f\"Dynamic Targets {title_suffix}\")\n",
    "    plt.xlabel(\"Predicted Class (Target)\")\n",
    "    plt.ylabel(\"True Class (Source)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    print(f\"Heatmap lagret til: {filename}\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. HOVEDLØKKE (RUNNER)\n",
    "# ==========================================\n",
    "for max_epochs in RUNS:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"  STARTER KJØRING: {max_epochs} EPOKER (Exp 4a)\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Initier for denne kjøringen\n",
    "    run_dir = BASE_ARTIFACTS_DIR / f\"run_{max_epochs}ep\"\n",
    "    run_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Last inn start-punktene fra Proben (T0)\n",
    "    payload = torch.load(TARGETS_PATH, map_location=device)\n",
    "    # Start med Probe logits\n",
    "    target_logits_matrix = payload[\"class_avg_logits\"].to(device)\n",
    "    # Lagre ankeret (T0)\n",
    "    target_logits_matrix_0 = target_logits_matrix.clone()\n",
    "    \n",
    "    print(f\"Start-targets lastet. Form: {target_logits_matrix.shape}\")\n",
    "\n",
    "    # Modell oppsett\n",
    "    model = make_cifar_resnet18(NUM_CLASSES).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    # Kriterium\n",
    "    criterion = DynamicTargetWMSE(ce_anchor=CE_ANCHOR, logit_l2=LOGIT_L2)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"test_acc\": [], \"target_updates\": []}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for ep in range(1, max_epochs + 1):\n",
    "        # --- STEG 1: Evaluer Presisjon & Oppdater Mål ---\n",
    "        if ep == 1:\n",
    "            # Epoke 1: Start \"kaldt\" med uniform presisjon (ingen endring)\n",
    "            current_acc = torch.ones(NUM_CLASSES, device=device) * 0.5\n",
    "            updates_count = 0\n",
    "        else:\n",
    "            # Mål faktisk nøyaktighet på eval-settet\n",
    "            current_acc = per_class_accuracy(model, train_eval_loader, NUM_CLASSES)\n",
    "            \n",
    "            # Oppdater Mål-matrisen (Eksperiment 4 Magien)\n",
    "            target_logits_matrix, updates_count = update_target_logits(\n",
    "                T=target_logits_matrix,\n",
    "                acc=current_acc,\n",
    "                T0=target_logits_matrix_0,\n",
    "                beta=AUTH_BETA,\n",
    "                lr=TARGET_UPDATE_LR,\n",
    "                kappa=KAPPA_PUSHBACK,\n",
    "                min_diff=MIN_ACC_DIFF,\n",
    "                anchor_gamma=TARGET_ANCHOR_GAMMA,\n",
    "                clamp_M=TARGET_CLAMP_M\n",
    "            )\n",
    "        \n",
    "        history[\"target_updates\"].append(updates_count)\n",
    "        \n",
    "        # Beregn vekter for WMSE basert på presisjon\n",
    "        weights = weights_from_acc(current_acc, gamma=WEIGHT_GAMMA, eps=WEIGHT_EPS)\n",
    "        \n",
    "        # --- STEG 2: Trening ---\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", enabled=USE_AMP):\n",
    "                logits = model(x)\n",
    "                # Send inn NÅVÆRENDE (dynamiske) targets og vekter\n",
    "                loss = criterion(logits, y, target_logits_matrix, weights)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping er viktig når målene flytter seg\n",
    "            if GRAD_CLIP_NORM > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "                \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "            \n",
    "        avg_loss = total_loss / count\n",
    "        \n",
    "        # --- STEG 3: Evaluering ---\n",
    "        val_acc = evaluate(model, test_loader)\n",
    "        \n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"test_acc\"].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), CKPT_DIR / f\"{EXPERIMENT_NAME}_{max_epochs}ep_best.pth\")\n",
    "            \n",
    "        # Logging\n",
    "        if ep % 10 == 0 or ep == 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Ep {ep:03d} | Loss: {avg_loss:.4f} | Test Acc: {val_acc:.4f} | \"\n",
    "                  f\"Updates: {updates_count} | T: {elapsed:.0f}s\")\n",
    "            \n",
    "            # Vis topp/bunn klasse for å se dynamikken\n",
    "            if ep > 1:\n",
    "                top_cls = CIFAR10_CLASSES[current_acc.argmax()]\n",
    "                bot_cls = CIFAR10_CLASSES[current_acc.argmin()]\n",
    "                print(f\"   -> Top: {top_cls} ({current_acc.max():.2f}), Bot: {bot_cls} ({current_acc.min():.2f})\")\n",
    "\n",
    "    # Lagre resultater\n",
    "    res_file = run_dir / \"results.json\"\n",
    "    with open(res_file, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"config\": {\n",
    "                \"epochs\": max_epochs,\n",
    "                \"auth_beta\": AUTH_BETA,\n",
    "                \"target_update_lr\": TARGET_UPDATE_LR,\n",
    "                \"min_acc_diff\": MIN_ACC_DIFF,\n",
    "                \"ce_anchor\": CE_ANCHOR\n",
    "            },\n",
    "            \"best_acc\": best_acc,\n",
    "            \"history\": history\n",
    "        }, f, indent=2)\n",
    "        \n",
    "    # Lagre Final Target Heatmap\n",
    "    save_target_heatmap(\n",
    "        target_logits_matrix, \n",
    "        CIFAR10_CLASSES, \n",
    "        run_dir / \"final_learned_distribution.png\",\n",
    "        title_suffix=f\"(Run {max_epochs} eps)\"\n",
    "    )\n",
    "    \n",
    "    # Lagre matrisen\n",
    "    torch.save(target_logits_matrix, run_dir / \"final_target_matrix.pt\")\n",
    "    \n",
    "    # Lagre plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Loss\")\n",
    "    plt.title(\"Train Loss\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"test_acc\"], label=\"Test Acc\", color=\"orange\")\n",
    "    plt.title(\"Test Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"training_metrics.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Ferdig med {max_epochs} epoker. Beste Acc: {best_acc:.4f}. Data lagret i {run_dir}\")\n",
    "\n",
    "print(f\"\\nAlle kjøringer fullført for {EXPERIMENT_NAME}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
