{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0a11c-da3d-4560-92e7-bff6292011af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1 (Complete Experiment 4b Code) ===\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASJON & OPPSETT\n",
    "# ==========================================\n",
    "EXPERIMENT_NAME = \"07_exp4b_dynamic_boost\"\n",
    "DATA_DIR = Path(\"./data\")\n",
    "BASE_ARTIFACTS_DIR = Path(f\"./artifacts/{EXPERIMENT_NAME}\")\n",
    "CKPT_DIR = Path(\"./checkpoints\")\n",
    "\n",
    "# Startpunkt: Vi bruker Logits lært av Proben (fra 01_setup_probe)\n",
    "TARGETS_PATH = Path(\"./artifacts/01_probe_baseline/probe_targets.pt\")\n",
    "\n",
    "# Opprett mapper\n",
    "BASE_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparametere (Generelt)\n",
    "RUNS = [50, 300]        # Kjører både 50 og 300 epoker\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "USE_AMP = True\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "# Hyperparametere (Dynamikk - Fra eks4_3)\n",
    "MIN_ACC_DIFF = 0.05         # Minimum forskjell i presisjon for at dynamikken skal slå inn\n",
    "AUTH_BETA = 10.0            # Sigmoid-skarphet for autoritet\n",
    "TARGET_UPDATE_LR = 0.10     # Steglengde for target-oppdatering\n",
    "KAPPA_PUSHBACK = 0.10       # \"Push\" på eksperten (repulsjon)\n",
    "TARGET_ANCHOR_GAMMA = 0.0   # VIKTIG: 0.0 i eks4_3 (ingen anker mot probe, lar targets flyte)\n",
    "TARGET_CLAMP_M = 10.0       # Clamp targets til [-M, M]\n",
    "\n",
    "# Hyperparametere (Vekting & Boost)\n",
    "WEIGHT_GAMMA = 1.0\n",
    "WEIGHT_EPS = 1e-6\n",
    "TRUE_CLASS_BOOST = 8.0      # VIKTIG: Korrekt klasse vektes 8x mer i Loss\n",
    "CE_ANCHOR = 0.05            # Lite anker for stabilitet\n",
    "LOGIT_L2 = 0.001            # L2 på logits for å hindre eksplosjon\n",
    "\n",
    "# Data & Klasser\n",
    "NUM_CLASSES = 10\n",
    "CIFAR10_CLASSES = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "print(f\"Konfigurasjon satt for {EXPERIMENT_NAME}.\")\n",
    "print(f\"Artefakter lagres til: {BASE_ARTIFACTS_DIR}\")\n",
    "\n",
    "if not TARGETS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Fant ikke teacher targets på {TARGETS_PATH}. Har du kjørt 01_setup_probe.ipynb?\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. REPRODUSERBARHET\n",
    "# ==========================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kjører på: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "eval_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_ds_aug = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_tf)\n",
    "# Eval-loader for training set (uten aug) for å måle presisjon (\"Autoritet\")\n",
    "train_ds_eval = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=eval_tf)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=eval_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds_aug, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "train_eval_loader = DataLoader(train_ds_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODELL DEFINISJON\n",
    "# ==========================================\n",
    "def make_cifar_resnet18(num_classes=10):\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "# ==========================================\n",
    "# 5. KJERNE-LOGIKK: DYNAMISK OPPDATERING\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def update_target_logits(\n",
    "    T, acc, T0=None, beta=10.0, lr=0.10, kappa=0.10, \n",
    "    min_diff=0.05, anchor_gamma=0.0, clamp_M=10.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Oppdaterer mål-matrisen T basert på asymmetrisk autoritet.\n",
    "    Identisk logikk som i 06_exp4a / eks4_2 / eks4_3.\n",
    "    \"\"\"\n",
    "    C = T.shape[0]\n",
    "    T_new = T.clone()\n",
    "    updates_made = 0\n",
    "\n",
    "    for i in range(C):          # i = novise\n",
    "        for j in range(C):      # j = ekspert\n",
    "            if i == j: continue\n",
    "\n",
    "            diff = acc[j] - acc[i]\n",
    "            if diff <= min_diff: continue\n",
    "\n",
    "            # Autoritet\n",
    "            p = torch.sigmoid(beta * diff)\n",
    "\n",
    "            # Ekspertens syn på relasjonen\n",
    "            expert_view = T[j, i]\n",
    "            novice_view = T[i, j]\n",
    "\n",
    "            # Trekk novisen mot eksperten\n",
    "            change = lr * p * (expert_view - novice_view)\n",
    "            T_new[i, j] += change\n",
    "\n",
    "            # Pushback (repulsjon) på eksperten\n",
    "            if kappa > 0:\n",
    "                T_new[j, i] -= kappa * change\n",
    "\n",
    "            updates_made += 1\n",
    "\n",
    "    # Anker (Her satt til 0.0 i dette eksperimentet for å la ting flyte)\n",
    "    if T0 is not None and anchor_gamma > 0:\n",
    "        T_new = (1.0 - anchor_gamma) * T_new + anchor_gamma * T0\n",
    "\n",
    "    # Clamp\n",
    "    if clamp_M is not None and clamp_M > 0:\n",
    "        T_new = T_new.clamp(min=-clamp_M, max=clamp_M)\n",
    "\n",
    "    return T_new, updates_made\n",
    "\n",
    "# ==========================================\n",
    "# 6. CUSTOM LOSS: BOOSTED DYNAMIC WMSE\n",
    "# ==========================================\n",
    "class DynamicTargetWMSE(nn.Module):\n",
    "    def __init__(self, ce_anchor=0.0, logit_l2=0.0, true_class_boost=1.0):\n",
    "        super().__init__()\n",
    "        self.ce_anchor = ce_anchor\n",
    "        self.logit_l2 = logit_l2\n",
    "        self.true_class_boost = float(true_class_boost)\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, y, current_targets, current_weights):\n",
    "        # 1. Hent targets for batchen basert på y\n",
    "        batch_targets = current_targets[y] # (B, C)\n",
    "        \n",
    "        # 2. WMSE Loss\n",
    "        diff = (logits - batch_targets) ** 2\n",
    "        \n",
    "        # Expand weights to batch: (1, C) -> (B, C)\n",
    "        B, C = logits.shape\n",
    "        w_batch = current_weights.view(1, -1).expand(B, -1).clone()\n",
    "        \n",
    "        # --- TRUE CLASS BOOST LOGIC ---\n",
    "        # Hvis boost er aktivert, øk vekten for den sanne klassen i hver sample\n",
    "        if self.true_class_boost != 1.0:\n",
    "            idx = torch.arange(B, device=logits.device)\n",
    "            w_batch[idx, y] *= self.true_class_boost\n",
    "        \n",
    "        weighted_mse = (diff * w_batch).mean()\n",
    "        loss = weighted_mse\n",
    "        \n",
    "        # 3. Stabilisering\n",
    "        if self.ce_anchor > 0:\n",
    "            loss += self.ce_anchor * self.ce(logits, y)\n",
    "            \n",
    "        if self.logit_l2 > 0:\n",
    "            loss += self.logit_l2 * (logits ** 2).mean()\n",
    "            \n",
    "        return loss\n",
    "\n",
    "# ==========================================\n",
    "# 7. HJELPEFUNKSJONER (Metrics & Viz)\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def per_class_accuracy(model, loader, num_classes=10):\n",
    "    model.eval()\n",
    "    correct = torch.zeros(num_classes, device=device)\n",
    "    counts = torch.zeros(num_classes, device=device)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        pred = model(x).argmax(dim=1)\n",
    "        for c in range(num_classes):\n",
    "            mask = (y == c)\n",
    "            if mask.any():\n",
    "                counts[c] += mask.sum()\n",
    "                correct[c] += (pred[mask] == c).sum()\n",
    "    acc = correct / counts.clamp_min(1)\n",
    "    return acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def weights_from_acc(acc, gamma=1.0, eps=1e-6):\n",
    "    w = (acc.clamp_min(0.0) + eps).pow(gamma)\n",
    "    w = w / w.mean().clamp_min(1e-8)\n",
    "    return w\n",
    "\n",
    "def save_target_heatmap(logits_matrix, classes, filename, title_suffix=\"\"):\n",
    "    probs = torch.softmax(logits_matrix, dim=1).cpu().numpy()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        probs, xticklabels=classes, yticklabels=classes,\n",
    "        annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
    "        cbar_kws={'label': 'Target Probability'}\n",
    "    )\n",
    "    plt.title(f\"Dynamic Targets {title_suffix}\")\n",
    "    plt.xlabel(\"Predicted Class (Target)\")\n",
    "    plt.ylabel(\"True Class (Source)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    print(f\"Heatmap lagret til: {filename}\")\n",
    "\n",
    "# ==========================================\n",
    "# 8. HOVEDLØKKE (RUNNER)\n",
    "# ==========================================\n",
    "for max_epochs in RUNS:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"  STARTER KJØRING: {max_epochs} EPOKER (Exp 4b - Boosted)\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Initier for denne kjøringen\n",
    "    run_dir = BASE_ARTIFACTS_DIR / f\"run_{max_epochs}ep\"\n",
    "    run_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Last inn start-punktene (Probe)\n",
    "    payload = torch.load(TARGETS_PATH, map_location=device)\n",
    "    target_logits_matrix = payload[\"class_avg_logits\"].to(device)\n",
    "    target_logits_matrix_0 = target_logits_matrix.clone()\n",
    "    \n",
    "    print(f\"Start-targets lastet. Form: {target_logits_matrix.shape}\")\n",
    "\n",
    "    # Modell\n",
    "    model = make_cifar_resnet18(NUM_CLASSES).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    # Kriterium med TRUE CLASS BOOST\n",
    "    criterion = DynamicTargetWMSE(\n",
    "        ce_anchor=CE_ANCHOR, \n",
    "        logit_l2=LOGIT_L2, \n",
    "        true_class_boost=TRUE_CLASS_BOOST\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"test_acc\": [], \"target_updates\": []}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for ep in range(1, max_epochs + 1):\n",
    "        # --- STEG 1: Evaluer Presisjon & Oppdater Mål ---\n",
    "        if ep == 1:\n",
    "            current_acc = torch.ones(NUM_CLASSES, device=device) * 0.5\n",
    "            updates_count = 0\n",
    "        else:\n",
    "            current_acc = per_class_accuracy(model, train_eval_loader, NUM_CLASSES)\n",
    "            \n",
    "            # Oppdater Mål-matrisen\n",
    "            target_logits_matrix, updates_count = update_target_logits(\n",
    "                T=target_logits_matrix,\n",
    "                acc=current_acc,\n",
    "                T0=target_logits_matrix_0,\n",
    "                beta=AUTH_BETA,\n",
    "                lr=TARGET_UPDATE_LR,\n",
    "                kappa=KAPPA_PUSHBACK,\n",
    "                min_diff=MIN_ACC_DIFF,\n",
    "                anchor_gamma=TARGET_ANCHOR_GAMMA, # Her er denne 0.0\n",
    "                clamp_M=TARGET_CLAMP_M\n",
    "            )\n",
    "        \n",
    "        history[\"target_updates\"].append(updates_count)\n",
    "        \n",
    "        # Beregn vekter\n",
    "        weights = weights_from_acc(current_acc, gamma=WEIGHT_GAMMA, eps=WEIGHT_EPS)\n",
    "        \n",
    "        # --- STEG 2: Trening ---\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", enabled=USE_AMP):\n",
    "                logits = model(x)\n",
    "                # Loss med boost\n",
    "                loss = criterion(logits, y, target_logits_matrix, weights)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if GRAD_CLIP_NORM > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "                \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "            \n",
    "        avg_loss = total_loss / count\n",
    "        \n",
    "        # --- STEG 3: Evaluering ---\n",
    "        val_acc = evaluate(model, test_loader)\n",
    "        \n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"test_acc\"].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), CKPT_DIR / f\"{EXPERIMENT_NAME}_{max_epochs}ep_best.pth\")\n",
    "            \n",
    "        # Logging\n",
    "        if ep % 10 == 0 or ep == 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Ep {ep:03d} | Loss: {avg_loss:.4f} | Test Acc: {val_acc:.4f} | \"\n",
    "                  f\"Updates: {updates_count} | T: {elapsed:.0f}s\")\n",
    "            \n",
    "            # Vis dynamikk (Top vs Bunn)\n",
    "            if ep > 1:\n",
    "                top_cls = CIFAR10_CLASSES[current_acc.argmax()]\n",
    "                bot_cls = CIFAR10_CLASSES[current_acc.argmin()]\n",
    "                print(f\"   -> Top: {top_cls} ({current_acc.max():.2f}), Bot: {bot_cls} ({current_acc.min():.2f})\")\n",
    "\n",
    "    # Lagre resultater\n",
    "    res_file = run_dir / \"results.json\"\n",
    "    with open(res_file, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"config\": {\n",
    "                \"epochs\": max_epochs,\n",
    "                \"auth_beta\": AUTH_BETA,\n",
    "                \"target_update_lr\": TARGET_UPDATE_LR,\n",
    "                \"true_class_boost\": TRUE_CLASS_BOOST,\n",
    "                \"anchor_gamma\": TARGET_ANCHOR_GAMMA\n",
    "            },\n",
    "            \"best_acc\": best_acc,\n",
    "            \"history\": history\n",
    "        }, f, indent=2)\n",
    "        \n",
    "    # Heatmap av slutt-tilstand\n",
    "    save_target_heatmap(\n",
    "        target_logits_matrix, \n",
    "        CIFAR10_CLASSES, \n",
    "        run_dir / \"final_learned_distribution.png\",\n",
    "        title_suffix=f\"(Run {max_epochs} eps - Boost {TRUE_CLASS_BOOST}x)\"\n",
    "    )\n",
    "    \n",
    "    # Lagre matrisen\n",
    "    torch.save(target_logits_matrix, run_dir / \"final_target_matrix.pt\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"test_acc\"], label=\"Test Acc\")\n",
    "    plt.title(f\"Training Progress (Boost={TRUE_CLASS_BOOST})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(run_dir / \"training_metrics.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Ferdig med {max_epochs} epoker. Beste Acc: {best_acc:.4f}. Data lagret i {run_dir}\")\n",
    "\n",
    "print(f\"\\nAlle kjøringer fullført for {EXPERIMENT_NAME}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
