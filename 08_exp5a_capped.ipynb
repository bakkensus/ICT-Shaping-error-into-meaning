{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82020800-f6f3-4749-8572-c5d94f6fc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1 (Complete Experiment 5a Code) ===\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASJON & OPPSETT\n",
    "# ==========================================\n",
    "EXPERIMENT_NAME = \"08_exp5a_capped\"\n",
    "DATA_DIR = Path(\"./data\")\n",
    "BASE_ARTIFACTS_DIR = Path(f\"./artifacts/{EXPERIMENT_NAME}\")\n",
    "CKPT_DIR = Path(\"./checkpoints\")\n",
    "\n",
    "# Startpunkt: Vi bruker Logits lært av Proben (fra 01_setup_probe)\n",
    "TARGETS_PATH = Path(\"./artifacts/01_probe_baseline/probe_targets.pt\")\n",
    "\n",
    "# Opprett mapper\n",
    "BASE_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Hyperparametere for Eksperimentet ---\n",
    "# Loop 1: Treningslengde\n",
    "RUNS_EPOCHS = [50, 300]\n",
    "\n",
    "# Loop 2: Probability Caps (Hvor sikker får modellen lov til å være på seg selv?)\n",
    "# 0.60 = Veldig usikker (myk), 0.80 = Balansert (original exp 5), 0.95 = Nesten One-Hot\n",
    "CAP_SETTINGS = [0.60, 0.80, 0.95] \n",
    "\n",
    "# Generelle treningsparametre\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "USE_AMP = True\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "# Parametere for Dynamikk (Arvet fra Exp 4b)\n",
    "MIN_ACC_DIFF = 0.05         # Min forskjell for autoritet\n",
    "AUTH_BETA = 10.0            # Sigmoid skarphet\n",
    "TARGET_UPDATE_LR = 0.10     # Steglengde for targets\n",
    "KAPPA_PUSHBACK = 0.10       # Repulsjon\n",
    "TARGET_ANCHOR_GAMMA = 0.0   # Ingen anker, la targets flyte (men begrenses av Cap)\n",
    "TARGET_CLAMP_M = 10.0       # Numerisk stabilitet\n",
    "\n",
    "# Parametere for Vekting & Boost (Arvet fra Exp 4b)\n",
    "WEIGHT_GAMMA = 1.0\n",
    "WEIGHT_EPS = 1e-6\n",
    "TRUE_CLASS_BOOST = 8.0      # Korrekt klasse vektes 8x mer i Loss\n",
    "CE_ANCHOR = 0.05            # Lite anker for stabilitet\n",
    "LOGIT_L2 = 0.001            # L2 på logits\n",
    "\n",
    "# Data & Klasser\n",
    "NUM_CLASSES = 10\n",
    "CIFAR10_CLASSES = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "print(f\"Konfigurasjon satt for {EXPERIMENT_NAME}.\")\n",
    "print(f\"Artefakter lagres til: {BASE_ARTIFACTS_DIR}\")\n",
    "\n",
    "if not TARGETS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Fant ikke teacher targets på {TARGETS_PATH}. Har du kjørt 01_setup_probe.ipynb?\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. REPRODUSERBARHET\n",
    "# ==========================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kjører på: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "eval_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_ds_aug = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_tf)\n",
    "# Eval-loader for training set (uten aug) for å måle presisjon (\"Autoritet\")\n",
    "train_ds_eval = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=eval_tf)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=eval_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds_aug, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "train_eval_loader = DataLoader(train_ds_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODELL DEFINISJON\n",
    "# ==========================================\n",
    "def make_cifar_resnet18(num_classes=10):\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "# ==========================================\n",
    "# 5. KJERNE-LOGIKK: CAPPING OG DYNAMIKK\n",
    "# ==========================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_probability_cap(logits_matrix, max_p=0.80):\n",
    "    \"\"\"\n",
    "    Tvinger diagonalen (seg selv) til å være maks 'max_p'.\n",
    "    Overskytende sannsynlighet fordeles proporsjonalt på de andre klassene (off-diagonal).\n",
    "    Returnerer logits (samme skala/magnitude som input).\n",
    "    \"\"\"\n",
    "    C = logits_matrix.shape[0]\n",
    "    probs = torch.softmax(logits_matrix, dim=1)\n",
    "    \n",
    "    for i in range(C):\n",
    "        p_self = probs[i, i]\n",
    "        \n",
    "        if p_self > max_p:\n",
    "            # 1. Sett taket\n",
    "            probs[i, i] = max_p\n",
    "            \n",
    "            # 2. Finn resten (off-diagonal)\n",
    "            others_mask = torch.ones(C, dtype=torch.bool, device=logits_matrix.device)\n",
    "            others_mask[i] = False\n",
    "            \n",
    "            current_others_sum = probs[i, others_mask].sum()\n",
    "            target_others_sum = 1.0 - max_p\n",
    "            \n",
    "            # 3. Skaler resten opp proporsjonalt\n",
    "            if current_others_sum > 1e-9:\n",
    "                scale = target_others_sum / current_others_sum\n",
    "                probs[i, others_mask] *= scale\n",
    "            else:\n",
    "                # Hvis de andre var 0, fordel uniformt\n",
    "                fill_val = target_others_sum / (C - 1)\n",
    "                probs[i, others_mask] = fill_val\n",
    "\n",
    "    # Konverter tilbake til logits, bevar dynamisk range\n",
    "    new_logits = torch.log(probs + 1e-9)\n",
    "    old_max_vals = logits_matrix.max(dim=1, keepdim=True).values\n",
    "    new_max_vals = new_logits.max(dim=1, keepdim=True).values\n",
    "    final_logits = new_logits - new_max_vals + old_max_vals\n",
    "    \n",
    "    return final_logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_target_logits(\n",
    "    T, acc, T0=None, beta=10.0, lr=0.10, kappa=0.10, \n",
    "    min_diff=0.05, anchor_gamma=0.0, clamp_M=10.0\n",
    "):\n",
    "    \"\"\"Dynamisk oppdatering av logits basert på presisjon.\"\"\"\n",
    "    C = T.shape[0]\n",
    "    T_new = T.clone()\n",
    "    updates_made = 0\n",
    "\n",
    "    for i in range(C):          # i = novise\n",
    "        for j in range(C):      # j = ekspert\n",
    "            if i == j: continue\n",
    "\n",
    "            diff = acc[j] - acc[i]\n",
    "            if diff <= min_diff: continue\n",
    "\n",
    "            p = torch.sigmoid(beta * diff)\n",
    "            expert_view = T[j, i]\n",
    "            novice_view = T[i, j]\n",
    "\n",
    "            change = lr * p * (expert_view - novice_view)\n",
    "            T_new[i, j] += change\n",
    "\n",
    "            if kappa > 0:\n",
    "                T_new[j, i] -= kappa * change # Pushback\n",
    "\n",
    "            updates_made += 1\n",
    "\n",
    "    if T0 is not None and anchor_gamma > 0:\n",
    "        T_new = (1.0 - anchor_gamma) * T_new + anchor_gamma * T0\n",
    "\n",
    "    if clamp_M is not None and clamp_M > 0:\n",
    "        T_new = T_new.clamp(min=-clamp_M, max=clamp_M)\n",
    "\n",
    "    return T_new, updates_made\n",
    "\n",
    "# ==========================================\n",
    "# 6. CUSTOM LOSS\n",
    "# ==========================================\n",
    "class DynamicTargetWMSE(nn.Module):\n",
    "    def __init__(self, ce_anchor=0.0, logit_l2=0.0, true_class_boost=1.0):\n",
    "        super().__init__()\n",
    "        self.ce_anchor = ce_anchor\n",
    "        self.logit_l2 = logit_l2\n",
    "        self.true_class_boost = float(true_class_boost)\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, y, current_targets, current_weights):\n",
    "        # Hent targets\n",
    "        batch_targets = current_targets[y]\n",
    "        \n",
    "        # WMSE\n",
    "        diff = (logits - batch_targets) ** 2\n",
    "        B, C = logits.shape\n",
    "        w_batch = current_weights.view(1, -1).expand(B, -1).clone()\n",
    "\n",
    "        # True Class Boost\n",
    "        if self.true_class_boost != 1.0:\n",
    "            idx = torch.arange(B, device=logits.device)\n",
    "            w_batch[idx, y] *= self.true_class_boost\n",
    "\n",
    "        weighted_mse = (diff * w_batch).mean()\n",
    "        loss = weighted_mse\n",
    "\n",
    "        # Stabilisering\n",
    "        if self.ce_anchor > 0:\n",
    "            loss += self.ce_anchor * self.ce(logits, y)\n",
    "        if self.logit_l2 > 0:\n",
    "            loss += self.logit_l2 * (logits ** 2).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# ==========================================\n",
    "# 7. HJELPEFUNKSJONER (Metrics & Viz)\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def per_class_accuracy(model, loader, num_classes=10):\n",
    "    model.eval()\n",
    "    correct = torch.zeros(num_classes, device=device)\n",
    "    counts = torch.zeros(num_classes, device=device)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        pred = model(x).argmax(dim=1)\n",
    "        for c in range(num_classes):\n",
    "            mask = (y == c)\n",
    "            if mask.any():\n",
    "                counts[c] += mask.sum()\n",
    "                correct[c] += (pred[mask] == c).sum()\n",
    "    acc = correct / counts.clamp_min(1)\n",
    "    return acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def weights_from_acc(acc, gamma=1.0, eps=1e-6):\n",
    "    w = (acc.clamp_min(0.0) + eps).pow(gamma)\n",
    "    w = w / w.mean().clamp_min(1e-8)\n",
    "    return w\n",
    "\n",
    "def save_target_heatmap(logits_matrix, classes, filename, title_suffix=\"\"):\n",
    "    probs = torch.softmax(logits_matrix, dim=1).cpu().numpy()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        probs, xticklabels=classes, yticklabels=classes,\n",
    "        annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
    "        cbar_kws={'label': 'Target Probability'}\n",
    "    )\n",
    "    plt.title(f\"Final Targets {title_suffix}\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    print(f\"Heatmap lagret til: {filename}\")\n",
    "\n",
    "# ==========================================\n",
    "# 8. HOVEDLØKKE (RUNNER)\n",
    "# ==========================================\n",
    "# Vi itererer over både Epoker og Probability Caps\n",
    "for max_epochs in RUNS_EPOCHS:\n",
    "    for cap in CAP_SETTINGS:\n",
    "        \n",
    "        run_name = f\"run_{max_epochs}ep_cap{cap}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  STARTER KJØRING: {max_epochs} EPOKER | CAP: {cap} (Max Self-Prob)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initier mappe for denne spesifikke kombinasjonen\n",
    "        run_dir = BASE_ARTIFACTS_DIR / run_name\n",
    "        run_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # 1. Last inn start-targets (Probe)\n",
    "        payload = torch.load(TARGETS_PATH, map_location=device)\n",
    "        target_logits_matrix = payload[\"class_avg_logits\"].to(device)\n",
    "        \n",
    "        # 2. PÅFØR CAP PÅ START-MATRISEN\n",
    "        # Dette er viktig for at utgangspunktet skal være \"gyldig\" iht reglene i eksperimentet\n",
    "        print(f\"Applierer Probability Cap ({cap}) på start-matrisen...\")\n",
    "        target_logits_matrix = apply_probability_cap(target_logits_matrix, max_p=cap)\n",
    "        target_logits_matrix_0 = target_logits_matrix.clone() # Anker (hvis aktivert, her 0.0)\n",
    "\n",
    "        # Modell & Optimizer\n",
    "        model = make_cifar_resnet18(NUM_CLASSES).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "        \n",
    "        # Kriterium\n",
    "        criterion = DynamicTargetWMSE(\n",
    "            ce_anchor=CE_ANCHOR, \n",
    "            logit_l2=LOGIT_L2, \n",
    "            true_class_boost=TRUE_CLASS_BOOST\n",
    "        )\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        history = {\"train_loss\": [], \"test_acc\": [], \"target_updates\": []}\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for ep in range(1, max_epochs + 1):\n",
    "            # --- FØR EPOKE: Evaluer & Oppdater Mål ---\n",
    "            if ep == 1:\n",
    "                current_acc = torch.ones(NUM_CLASSES, device=device) * 0.5\n",
    "                updates_count = 0\n",
    "            else:\n",
    "                current_acc = per_class_accuracy(model, train_eval_loader, NUM_CLASSES)\n",
    "                \n",
    "                # A. Dynamisk Oppdatering (Basert på presisjon)\n",
    "                target_logits_matrix, updates_count = update_target_logits(\n",
    "                    T=target_logits_matrix,\n",
    "                    acc=current_acc,\n",
    "                    T0=target_logits_matrix_0,\n",
    "                    beta=AUTH_BETA,\n",
    "                    lr=TARGET_UPDATE_LR,\n",
    "                    kappa=KAPPA_PUSHBACK,\n",
    "                    min_diff=MIN_ACC_DIFF,\n",
    "                    anchor_gamma=TARGET_ANCHOR_GAMMA,\n",
    "                    clamp_M=TARGET_CLAMP_M\n",
    "                )\n",
    "                \n",
    "                # B. Tvungen Capping (Etter dynamikken kan ting ha endret seg, tving cap på nytt)\n",
    "                target_logits_matrix = apply_probability_cap(target_logits_matrix, max_p=cap)\n",
    "            \n",
    "            history[\"target_updates\"].append(updates_count)\n",
    "            \n",
    "            # Beregn vekter for WMSE\n",
    "            weights = weights_from_acc(current_acc, gamma=WEIGHT_GAMMA, eps=WEIGHT_EPS)\n",
    "            \n",
    "            # --- TRENING ---\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            \n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with torch.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", enabled=USE_AMP):\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits, y, target_logits_matrix, weights)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                if GRAD_CLIP_NORM > 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            \n",
    "            avg_loss = total_loss / count\n",
    "            \n",
    "            # --- EVALUERING ---\n",
    "            val_acc = evaluate(model, test_loader)\n",
    "            history[\"train_loss\"].append(avg_loss)\n",
    "            history[\"test_acc\"].append(val_acc)\n",
    "            \n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), CKPT_DIR / f\"{EXPERIMENT_NAME}_{run_name}_best.pth\")\n",
    "            \n",
    "            # Logging\n",
    "            if ep % 10 == 0 or ep == 1:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Ep {ep:03d} | Loss: {avg_loss:.4f} | Test Acc: {val_acc:.4f} | Upd: {updates_count} | T: {elapsed:.0f}s\")\n",
    "                \n",
    "                # Sjekk at Capping faktisk virker (print max self-prob)\n",
    "                if ep % 20 == 0:\n",
    "                    probs = torch.softmax(target_logits_matrix, dim=1)\n",
    "                    max_observed = probs.diagonal().max().item()\n",
    "                    print(f\"   -> Max Observed Self-Prob: {max_observed:.4f} (Cap: {cap})\")\n",
    "\n",
    "        # --- ETTER KJØRING: LAGRE ALT ---\n",
    "        \n",
    "        # 1. Resultater JSON\n",
    "        res_file = run_dir / \"results.json\"\n",
    "        with open(res_file, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"config\": {\n",
    "                    \"epochs\": max_epochs,\n",
    "                    \"max_self_prob\": cap,\n",
    "                    \"auth_beta\": AUTH_BETA,\n",
    "                    \"true_class_boost\": TRUE_CLASS_BOOST,\n",
    "                    \"lr\": LR\n",
    "                },\n",
    "                \"best_acc\": best_acc,\n",
    "                \"history\": history\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        # 2. Heatmap av slutt-distribusjon\n",
    "        save_target_heatmap(\n",
    "            target_logits_matrix, \n",
    "            CIFAR10_CLASSES, \n",
    "            run_dir / \"final_learned_distribution.png\",\n",
    "            title_suffix=f\"(Run {max_epochs}ep, Cap {cap})\"\n",
    "        )\n",
    "        \n",
    "        # 3. Lagre selve matrisen\n",
    "        torch.save(target_logits_matrix, run_dir / \"final_target_matrix.pt\")\n",
    "        \n",
    "        # 4. Treningsplot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history[\"test_acc\"], label=f\"Test Acc (Cap {cap})\")\n",
    "        plt.title(f\"Training Progress ({max_epochs} epochs)\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(run_dir / \"training_metrics.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Ferdig med {run_name}. Beste Acc: {best_acc:.4f}. Resultater i {run_dir}\")\n",
    "\n",
    "print(f\"\\nAlle kjøringer fullført for {EXPERIMENT_NAME}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
