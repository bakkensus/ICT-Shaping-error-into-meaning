{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db319e-6aff-48b6-9c7a-96c0798fc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1 (Complete Experiment 3b Code) ===\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASJON & OPPSETT\n",
    "# ==========================================\n",
    "EXPERIMENT_NAME = \"05_exp3b_static_logits\"\n",
    "DATA_DIR = Path(\"./data\")\n",
    "BASE_ARTIFACTS_DIR = Path(f\"./artifacts/{EXPERIMENT_NAME}\")\n",
    "CKPT_DIR = Path(\"./checkpoints\")\n",
    "\n",
    "# Sti til Teacher Targets (fra 01_setup_probe)\n",
    "# Denne filen inneholder \"class_avg_logits\" som vi trenger her\n",
    "TARGETS_PATH = Path(\"./artifacts/01_probe_baseline/probe_targets.pt\")\n",
    "\n",
    "# Opprett mapper\n",
    "BASE_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparametere\n",
    "RUNS = [50, 300]        # Kjører både 50 og 300 epoker\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "USE_AMP = True\n",
    "GRAD_CLIP_NORM = 1.0    # Hindrer gradient explosion ved WMSE\n",
    "\n",
    "# Exp 3b Spesifikke parametere (fra gammel kode)\n",
    "# WMSE weighting from per-class accuracy\n",
    "WEIGHT_GAMMA = 1.0      # >1 emphasizes strong classes even more\n",
    "WEIGHT_EPS = 1e-6       # numerical floor before normalization\n",
    "REWEIGHT_EVERY_EPOCH = True # update weights once per epoch based on validation acc\n",
    "\n",
    "# Stabilizers\n",
    "CE_ANCHOR = 0.05        # small CE anchor (5%) for stability\n",
    "LOGIT_L2 = 0.001        # small L2 on logits to avoid value drift/explosion\n",
    "CENTER_INPUTS = False   # Keep False as per original best runs\n",
    "\n",
    "# Data & Klasser\n",
    "NUM_CLASSES = 10\n",
    "CIFAR10_CLASSES = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "print(f\"Konfigurasjon satt for {EXPERIMENT_NAME}.\")\n",
    "print(f\"Artefakter lagres til: {BASE_ARTIFACTS_DIR}\")\n",
    "\n",
    "if not TARGETS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Fant ikke teacher targets på {TARGETS_PATH}. Har du kjørt 01_setup_probe.ipynb?\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. REPRODUSERBARHET\n",
    "# ==========================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kjører på: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "train_tf = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "eval_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_ds_aug = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_tf)\n",
    "# Vi trenger train_ds_eval (uten aug) for å måle per-class accuracy presist for vekting\n",
    "train_ds_eval = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=eval_tf)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=eval_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds_aug, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "train_eval_loader = DataLoader(train_ds_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODELL DEFINISJON\n",
    "# ==========================================\n",
    "def make_cifar_resnet18(num_classes=10):\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "# ==========================================\n",
    "# 5. CUSTOM LOSS: LOGIT WMSE (Exp 3b)\n",
    "# ==========================================\n",
    "class LogitTargetWMSENLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted MSE between model logits and class-conditional average logit targets.\n",
    "    \n",
    "    Given:\n",
    "      - target_table: (C, C) average logits from probe (row = true class, col = logit-dim)\n",
    "      - weights: (C,) per-dimension weights (one per class/logit dimension), updated externally\n",
    "    \"\"\"\n",
    "    def __init__(self, target_table: torch.Tensor, \n",
    "                 center_inputs: bool = False, \n",
    "                 ce_anchor: float = 0.0, \n",
    "                 logit_l2: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"T\", target_table)  # C x C\n",
    "        self.center_inputs = bool(center_inputs)\n",
    "        self.ce_anchor = float(ce_anchor)\n",
    "        self.logit_l2 = float(logit_l2)\n",
    "        self._weights = None  # device tensor (C,)\n",
    "        self.ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def set_weights(self, w: torch.Tensor):\n",
    "        # Normalize weights to mean 1 (keeps scale consistent across epochs)\n",
    "        w = w / w.mean().clamp_min(1e-8)\n",
    "        self._weights = w.detach()\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        assert self._weights is not None, \"Call set_weights(w) before forward.\"\n",
    "        \n",
    "        # 1. Hent targets for batchen\n",
    "        targets = self.T[y]  # (B, C)\n",
    "        x = logits\n",
    "        \n",
    "        if self.center_inputs:\n",
    "            x = x - x.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # 2. Weighted MSE across dimensions (classes)\n",
    "        # loss_i = mean_c w_c * (x_ic - t_ic)^2\n",
    "        diff2 = (x - targets) ** 2\n",
    "        weighted = diff2 * self._weights.view(1, -1)\n",
    "        wmse = weighted.mean()\n",
    "\n",
    "        # 3. Logit L2 Regularization (hindrer eksplosjon)\n",
    "        if self.logit_l2 > 0.0:\n",
    "            wmse = wmse + self.logit_l2 * (x.pow(2).mean())\n",
    "\n",
    "        # 4. CE Anchor (Stabilisering)\n",
    "        if self.ce_anchor > 0.0:\n",
    "            wmse = wmse + self.ce_anchor * self.ce_loss_fn(logits, y)\n",
    "\n",
    "        return wmse\n",
    "\n",
    "# ==========================================\n",
    "# 6. HJELPEFUNKSJONER (Vekting & Metrics)\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def per_class_accuracy(model, loader, num_classes=NUM_CLASSES):\n",
    "    \"\"\"Beregner accuracy per klasse. Brukes for å vekte WMSE.\"\"\"\n",
    "    model.eval()\n",
    "    correct = torch.zeros(num_classes, device=device)\n",
    "    counts  = torch.zeros(num_classes, device=device)\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            mask = (y == c)\n",
    "            if mask.any():\n",
    "                counts[c] += mask.sum()\n",
    "                correct[c] += (pred[mask] == c).sum()\n",
    "                \n",
    "    acc = correct.float() / counts.clamp_min(1)\n",
    "    return acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def weights_from_acc(acc: torch.Tensor, gamma: float = 1.0, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    Konverterer accuracy til vekter. Høyere accuracy -> Høyere vekt.\n",
    "    Dette gjør at vi stoler mer på logit-verdier for klasser modellen mestrer.\n",
    "    \"\"\"\n",
    "    base = (acc.clamp_min(0.0) + eps).pow(gamma)\n",
    "    w = base / base.mean().clamp_min(1e-8)\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def uniform_weights(num_classes=NUM_CLASSES):\n",
    "    return torch.ones(num_classes, dtype=torch.float32, device=device)\n",
    "\n",
    "# ==========================================\n",
    "# 7. LAST INN OG VISUALISER TARGETS\n",
    "# ==========================================\n",
    "payload = torch.load(TARGETS_PATH, map_location=device)\n",
    "# VIKTIG: Exp 3b bruker LOGITS, ikke PROBS\n",
    "class_avg_logits = payload[\"class_avg_logits\"] \n",
    "\n",
    "print(f\"Lastet logit-targets med form: {class_avg_logits.shape}\")\n",
    "\n",
    "# Visualiser Logits (OBS: Logits kan være negative)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    class_avg_logits.cpu().numpy(),\n",
    "    xticklabels=CIFAR10_CLASSES,\n",
    "    yticklabels=CIFAR10_CLASSES,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\", # Coolwarm er bra for positive/negative verdier\n",
    "    center=0.0,\n",
    "    cbar_kws={'label': 'Target Logits (Teacher Avg)'}\n",
    ")\n",
    "plt.title(f\"Experiment 3b Targets: Static Logits\")\n",
    "plt.xlabel(\"Predicted Logit Dimension\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.tight_layout()\n",
    "\n",
    "heatmap_path = BASE_ARTIFACTS_DIR / \"target_logits_heatmap.png\"\n",
    "plt.savefig(heatmap_path)\n",
    "print(f\"Heatmap lagret til: {heatmap_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 8. TRENINGSFUNKSJONER\n",
    "# ==========================================\n",
    "def train_one_epoch(model, loader, optimizer, scaler, criterion, grad_clip=0.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\" if device.type == \"cuda\" else \"cpu\", enabled=USE_AMP):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if grad_clip > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct_top1 = 0\n",
    "    correct_top2 = 0\n",
    "    ranks_sum = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Top-k\n",
    "        top2 = probs.topk(2, dim=1).indices\n",
    "        pred_top1 = top2[:, 0]\n",
    "        correct_top1 += (pred_top1 == y).sum().item()\n",
    "        correct_top2 += ((top2[:, 0] == y) | (top2[:, 1] == y)).sum().item()\n",
    "        \n",
    "        # Mean Rank\n",
    "        sorted_indices = logits.argsort(dim=1, descending=True)\n",
    "        ranks = (sorted_indices == y.view(-1, 1)).nonzero()[:, 1] + 1\n",
    "        ranks_sum += ranks.float().sum().item()\n",
    "        \n",
    "        total += x.size(0)\n",
    "        \n",
    "    return {\n",
    "        \"acc1\": correct_top1 / total,\n",
    "        \"acc2\": correct_top2 / total,\n",
    "        \"mean_rank\": ranks_sum / total\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 9. HOVEDLØKKE (RUNNER)\n",
    "# ==========================================\n",
    "for max_epochs in RUNS:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"  STARTER KJØRING: {max_epochs} EPOKER\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Initier for denne kjøringen\n",
    "    run_dir = BASE_ARTIFACTS_DIR / f\"run_{max_epochs}ep\"\n",
    "    run_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model = make_cifar_resnet18(NUM_CLASSES).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    # Initialiser Criterion med target-tabellen\n",
    "    criterion = LogitTargetWMSENLoss(\n",
    "        target_table=class_avg_logits,\n",
    "        center_inputs=CENTER_INPUTS,\n",
    "        ce_anchor=CE_ANCHOR,\n",
    "        logit_l2=LOGIT_L2\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"test_acc1\": [], \"test_acc2\": [], \"mean_rank\": []}\n",
    "    weights_history = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # 1. Oppdater vekter basert på presisjon\n",
    "        if REWEIGHT_EVERY_EPOCH and epoch > 1:\n",
    "            # Vi bruker train_eval_loader for å se hva modellen faktisk kan på treningsdataene uten støy\n",
    "            acc_c = per_class_accuracy(model, train_eval_loader, NUM_CLASSES)\n",
    "            w = weights_from_acc(acc_c, gamma=WEIGHT_GAMMA, eps=WEIGHT_EPS)\n",
    "        else:\n",
    "            w = uniform_weights(NUM_CLASSES)\n",
    "            \n",
    "        criterion.set_weights(w)\n",
    "        weights_history.append(w.detach().cpu().tolist())\n",
    "        \n",
    "        # 2. Tren\n",
    "        t_loss, t_acc = train_one_epoch(model, train_loader, optimizer, scaler, criterion, grad_clip=GRAD_CLIP_NORM)\n",
    "        \n",
    "        # 3. Evaluer\n",
    "        metrics = evaluate(model, test_loader)\n",
    "        \n",
    "        history[\"train_loss\"].append(t_loss)\n",
    "        history[\"train_acc\"].append(t_acc)\n",
    "        history[\"test_acc1\"].append(metrics[\"acc1\"])\n",
    "        history[\"test_acc2\"].append(metrics[\"acc2\"])\n",
    "        history[\"mean_rank\"].append(metrics[\"mean_rank\"])\n",
    "        \n",
    "        if metrics[\"acc1\"] > best_acc:\n",
    "            best_acc = metrics[\"acc1\"]\n",
    "            torch.save(model.state_dict(), CKPT_DIR / f\"{EXPERIMENT_NAME}_{max_epochs}ep_best.pth\")\n",
    "            \n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            # Logg også gjennomsnittsvekten for å se at systemet lever\n",
    "            mean_w = w.mean().item()\n",
    "            print(f\"Ep {epoch:03d} | Loss: {t_loss:.4f} | TrAcc: {t_acc:.3f} | \"\n",
    "                  f\"TeAcc1: {metrics['acc1']:.3f} | Rank: {metrics['mean_rank']:.2f} | \"\n",
    "                  f\"W_mean: {mean_w:.2f} | T: {elapsed:.0f}s\")\n",
    "            \n",
    "    # Lagre resultater\n",
    "    res_file = run_dir / \"results.json\"\n",
    "    with open(res_file, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"config\": {\n",
    "                \"epochs\": max_epochs,\n",
    "                \"weight_gamma\": WEIGHT_GAMMA,\n",
    "                \"ce_anchor\": CE_ANCHOR,\n",
    "                \"logit_l2\": LOGIT_L2\n",
    "            },\n",
    "            \"best_acc\": best_acc,\n",
    "            \"history\": history,\n",
    "            \"final_weights\": weights_history[-1] # Lagre siste vekting\n",
    "        }, f, indent=2)\n",
    "        \n",
    "    # Lagre vekthistorikk separat\n",
    "    with open(run_dir / \"weights_history.json\", \"w\") as f:\n",
    "        json.dump(weights_history, f)\n",
    "        \n",
    "    # Lagre plott\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss (WMSE)\")\n",
    "    plt.title(f\"Training Loss ({max_epochs} epochs) - Exp 3b\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(run_dir / \"loss_curve.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Ferdig med {max_epochs} epoker. Beste Acc: {best_acc:.4f}. Data lagret i {run_dir}\")\n",
    "\n",
    "print(f\"\\nAlle kjøringer fullført for {EXPERIMENT_NAME}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
